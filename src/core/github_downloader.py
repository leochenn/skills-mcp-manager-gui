import os

import requests


class GitHubDownloader:
    def __init__(self, log_callback):
        self.log_callback = log_callback
        self.stop_flag = False

    def download(self, github_url, output_dir):
        parts = github_url.strip("/").split("/")
        if "github.com" not in parts:
            self.log_callback("é”™è¯¯: ä¸æ˜¯æœ‰æ•ˆçš„ GitHub URL", "error")
            return False

        try:
            owner = parts[3]
            repo = parts[4]
            branch = parts[6]
            folder_path = "/".join(parts[7:])
            skill_name = folder_path.split("/")[-1]
        except IndexError:
            self.log_callback("é”™è¯¯: URL æ ¼å¼è§£æå¤±è´¥ï¼Œè¯·ç¡®ä¿åŒ…å« tree/{branch}/ç›®å½•è·¯å¾„", "error")
            return False

        api_url = f"https://api.github.com/repos/{owner}/{repo}/contents/{folder_path}?ref={branch}"
        base_dest_dir = os.path.join(output_dir, owner)

        try:
            self.log_callback(f"æ­£åœ¨åˆ†æç›®å½•ç»“æ„: {owner}/{repo}/{folder_path}", "info")
            self._smart_download(api_url, base_dest_dir, is_root=True, root_name=skill_name)
            self._record_address(base_dest_dir, github_url)
            self.log_callback("æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆï¼", "success")
            return True
        except Exception as e:
            self.log_callback(f"ä¸‹è½½è¿‡ç¨‹å‡ºé”™: {e}", "error")
            return False

    def _smart_download(self, api_url, base_dest_dir, is_root=False, root_name=None):
        self._smart_download_recursive(api_url, base_dest_dir, current_dir_name=root_name)

    def _smart_download_recursive(self, api_url, base_dest_dir, current_dir_name=None):
        if self.stop_flag:
            return

        headers = {"User-Agent": "Mozilla/5.0"}
        try:
            response = requests.get(api_url, headers=headers)
            if response.status_code != 200:
                self.log_callback(f"Error fetching {api_url}", "error")
                return

            items = response.json()
            if isinstance(items, dict) and items.get("type") == "file":
                items = [items]

            has_skill_md = any(item["name"].lower() == "skill.md" for item in items)

            if has_skill_md:
                if not current_dir_name:
                    self.log_callback("Error: Skill found but name unknown", "error")
                    return

                target_path = os.path.join(base_dest_dir, current_dir_name)
                self.log_callback(f"å‘ç° Skill: {current_dir_name}", "success")
                self.log_callback(f"ç›®æ ‡è·¯å¾„: {target_path}", "info")

                if not os.path.exists(target_path):
                    os.makedirs(target_path)

                self._download_items(items, target_path)
                return

            for item in items:
                if self.stop_flag:
                    return
                if item["type"] == "dir":
                    self._smart_download_recursive(item["url"], base_dest_dir, item["name"])

        except Exception as e:
            self.log_callback(f"Error: {e}", "error")

    def _download_items(self, items, local_path):
        headers = {"User-Agent": "Mozilla/5.0"}
        for item in items:
            if self.stop_flag:
                return

            name = item["name"]
            path = os.path.join(local_path, name)

            if item["type"] == "dir":
                if not os.path.exists(path):
                    os.makedirs(path)
                self._download_recursive(item["url"], path, "")
            else:
                self.log_callback(f"â¬‡ï¸  æ­£åœ¨ä¸‹è½½: {name}...", "file_start")
                resp = requests.get(item["download_url"], headers=headers)
                with open(path, "wb") as f:
                    f.write(resp.content)
                self.log_callback(f"ä¸‹è½½å®Œæˆ: {name}", "success")

    def _record_address(self, owner_dir, url):
        try:
            if not os.path.exists(owner_dir):
                os.makedirs(owner_dir)
            file_path = os.path.join(owner_dir, "github_address.txt")
            existing_urls = []

            if os.path.exists(file_path):
                with open(file_path, "r", encoding="utf-8") as f:
                    existing_urls = [line.strip() for line in f.readlines() if line.strip()]

            url = url.rstrip("/")

            def is_parent_of(parent, child):
                return child.startswith(parent + "/")

            def get_parent_url(u):
                return u.rsplit("/", 1)[0]

            is_covered = False
            for ex in existing_urls:
                if ex == url or is_parent_of(ex, url):
                    is_covered = True
                    break

            if is_covered:
                self.log_callback("åœ°å€å·²å­˜åœ¨æˆ–è¢«çˆ¶çº§åŒ…å«ï¼Œè·³è¿‡è®°å½•ã€‚", "info")
                return

            current_urls = existing_urls + [url]

            has_changed = True
            while has_changed:
                has_changed = False
                temp_list = sorted(list(set(current_urls)))

                parents = set()
                for u in temp_list:
                    is_child = False
                    for other in temp_list:
                        if u != other and is_parent_of(other, u):
                            is_child = True
                            break
                    if not is_child:
                        parents.add(u)

                groups = {}
                for u in parents:
                    p_url = get_parent_url(u)
                    groups.setdefault(p_url, []).append(u)

                final_round_urls = []
                for p_url, children in groups.items():
                    if len(children) > 1:
                        final_round_urls.append(p_url)
                        has_changed = True
                    else:
                        final_round_urls.append(children[0])

                if has_changed:
                    current_urls = final_round_urls
                else:
                    current_urls = list(parents)

            current_urls.sort()
            existing_urls.sort()

            if current_urls != existing_urls:
                with open(file_path, "w", encoding="utf-8") as f:
                    for u in current_urls:
                        f.write(u + "\n")
                self.log_callback(f"åœ°å€è®°å½•å·²æ›´æ–° (å·²åˆå¹¶/å»é‡): {file_path}", "info")
            else:
                self.log_callback("åœ°å€è®°å½•æ— éœ€æ›´æ–°ã€‚", "info")

        except Exception as e:
            self.log_callback(f"æ— æ³•å†™å…¥åœ°å€æ–‡ä»¶: {e}", "error")

    def _download_recursive(self, api_url, local_base_path, relative_path):
        if self.stop_flag:
            return

        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(api_url, headers=headers)
        if response.status_code != 200:
            self.log_callback(
                f"è·å–ç›®å½•ä¿¡æ¯å¤±è´¥: {api_url} (ä»£ç : {response.status_code})",
                "error",
            )
            return

        data = response.json()
        if isinstance(data, dict) and data.get("type") == "file":
            data = [data]

        for item in data:
            if self.stop_flag:
                return

            item_type = item["type"]
            item_name = item["name"]
            current_local_path = os.path.join(local_base_path, item_name)

            if item_type == "dir":
                if not os.path.exists(current_local_path):
                    os.makedirs(current_local_path)
                    self.log_callback(f"ğŸ“ åˆ›å»ºç›®å½•: {item_name}", "dir")

                self._download_recursive(item["url"], current_local_path, "")

            elif item_type == "file":
                download_url = item["download_url"]
                self.log_callback(f"â¬‡ï¸  æ­£åœ¨ä¸‹è½½: {item_name}...", "file_start")

                file_resp = requests.get(download_url, headers=headers)
                with open(current_local_path, "wb") as f:
                    f.write(file_resp.content)

                self.log_callback(f"ä¸‹è½½å®Œæˆ: {item_name}", "success")

